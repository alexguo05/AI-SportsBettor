# AI-SportsBettor

A disciplined, ToS-safe pipeline that ingests NFL news via RSS, captures licensed sportsbook odds snapshots, and produces latency-aware datasets for modeling line moves and generating manual betting alerts (human-in-the-loop only).

## ğŸ¯ Project Goals

- Ingest NFL news from RSS feeds with proper timestamp handling
- Capture licensed odds snapshots (no scraping)
- Time-align news to pre-news baseline prices
- Generate manual betting alerts (no automated wagering)
- Maintain full audit trail with raw data preservation

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11+
- pip or uv package manager
- (Optional) PostgreSQL 16 for database storage

### Installation

```bash
# Clone the repository
cd AI-SportsBettor

# Install dependencies
pip install -e .

# (Optional) Copy and configure environment
cp .env.example .env
# Edit .env with your configuration
```

### Running RSS News Ingestion

```bash
# Fetch NFL news from RSS feeds
python -m src.ingest_news.rss_pull
```

This will:
- Pull from 5 major NFL news sources (ESPN, CBS, PFT, NFL.com, SI)
- Normalize all timestamps to UTC
- Clean HTML content from articles
- Save raw JSONL data to `data/raw/news/YYYY-MM-DD/`
- Print summary statistics

## ğŸ“ Project Structure

```
AI-SportsBettor/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingest_news/      # RSS news ingestion
â”‚   â”‚   â”œâ”€â”€ rss_pull.py   # Main RSS fetching script
â”‚   â”‚   â””â”€â”€ README.md     # News ingestion docs
â”‚   â”œâ”€â”€ ingest_odds/      # Odds API client (TODO)
â”‚   â”œâ”€â”€ features/         # Entity resolution & linking (TODO)
â”‚   â””â”€â”€ common/           # Shared utilities & config
â”‚       â”œâ”€â”€ settings.py   # Configuration management
â”‚       â””â”€â”€ logging_config.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ news/         # Raw news JSONL files
â”‚   â”‚   â””â”€â”€ odds/         # Raw odds JSONL files
â”‚   â””â”€â”€ ref/              # Reference data (teams, schedules)
â”œâ”€â”€ infra/                # Docker Compose, migrations (TODO)
â”œâ”€â”€ tests/                # Unit tests (TODO)
â””â”€â”€ notebooks/            # Analysis notebooks (TODO)
```

## ğŸ”‘ Key Features (Current)

### RSS News Ingestion âœ…

- **Multiple Sources**: ESPN, CBS Sports, Pro Football Talk, NFL.com, Sports Illustrated
- **Time Truth**: Captures `published`, `updated`, and `first_seen` timestamps
- **UTC Everywhere**: All timestamps normalized to UTC
- **Content Deduplication**: SHA256 hash-based deduplication
- **Audit Trail**: Raw JSONL files with complete provenance
- **HTML Cleaning**: BeautifulSoup removes tags and normalizes whitespace

## ğŸš§ Roadmap

### Week 1 (MVP)
- [x] RSS news ingestion
- [ ] PostgreSQL database setup (Docker Compose)
- [ ] Database schema (Alembic migrations)
- [ ] Odds API integration (The Odds API)
- [ ] News-to-event linking
- [ ] APScheduler automation

### Week 2+
- [ ] Team/entity normalization
- [ ] Weather data integration
- [ ] Player props
- [ ] Closing line value (CLV) metrics
- [ ] Basic line movement models
- [ ] Alert generation system

## ğŸ›¡ï¸ Non-Negotiables

- **ToS-Safe**: Only licensed APIs, no sportsbook scraping
- **UTC Everywhere**: All timestamps in UTC
- **Audit Trail**: Raw payloads stored on disk
- **Human-in-the-Loop**: No automated wagering
- **Time Truth**: Proper event-time handling with pre-news baselines

## ğŸ“Š Data Flow (Planned)

```
RSS Feeds â†’ Raw JSONL â†’ news_events table
                              â†“
                         Entity Resolution
                              â†“
Odds API â†’ Raw JSONL â†’ odds_snapshots table
                              â†“
                    Time-Aligned Linking
                              â†“
                     news_odds_links table
                              â†“
                      Feature Engineering
                              â†“
                          Modeling
                              â†“
                     Manual Alerts ğŸš¨
```

## ğŸ“ Example Usage

### Fetch Latest NFL News

```python
from src.ingest_news.rss_pull import pull_all_feeds, save_to_jsonl
from src.common.settings import settings

# Pull all configured feeds
entries = pull_all_feeds()

# Save to disk
save_to_jsonl(entries, settings.data_raw_news_dir)

print(f"Collected {len(entries)} news entries")
```

## ğŸ¤ Contributing

This is a personal/solo MVP project. Focus areas:
1. Keep it simple and maintainable
2. Typed Python with Pydantic models
3. Small, testable modules
4. UTC timestamps everywhere
5. Preserve raw data for audit

## ğŸ“„ License

Private project for educational/research purposes only.

## âš ï¸ Disclaimer

This tool is for **educational and research purposes only**. It does not place bets automatically and is not financial advice. Sports betting carries risk. Never bet more than you can afford to lose.